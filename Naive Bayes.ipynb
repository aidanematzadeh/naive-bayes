{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprossing the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1 text ['Mortar', 'assault', 'leaves', 'at', 'least', '18', 'dead']\n",
      "id 2 text ['Goal', 'delight', 'for', 'Sheva']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# To remove punctuations\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "path = 'data/affectivetext_trial.xml'\n",
    "training_data = {}\n",
    "\n",
    "# Read the training data\n",
    "# Example data point <instance id=\"1\">Mortar assault leaves at least 18 dead</instance>\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        if \"instance\" in line:\n",
    "            line_id = int(line[line.find(\"id=\")+3:line.find(\">\")].strip(\"\\\"\"))          \n",
    "            line = line[line.find(\">\")+1:line.find(\"</\")]\n",
    "            \n",
    "            # Removing punctuations\n",
    "            line = line.translate(translator)\n",
    "        \n",
    "            if line_id <= 2:\n",
    "               print(\"id %s text %s\" % (line_id, line.split()))\n",
    "            \n",
    "            # Keep the lower-cased words of each document\n",
    "            training_data[line_id] = [word.lower() for word in line.split()]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the gold labels: Valence \n",
    "valence_labels = {}\n",
    "path = \"data/affectivetext_trial.valence.gold\"\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        line_id, valence = line.split()\n",
    "        valence = int(valence)\n",
    "        line_id = int(line_id)\n",
    "        # Discretize the scores. Use scores for regression. #TODO\n",
    "        if valence > 0: # positive \n",
    "            valence_labels[line_id] = 1\n",
    "        else: \n",
    "            valence_labels[line_id] = -1\n",
    "        \n",
    "#print(valence_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types 1004 \n",
      "\n",
      "C -1, prior prob. -0.47\n",
      "Number of word types in c 667\n",
      "C 1, prior prob. -0.99\n",
      "Number of word types in c 422\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_bayes(documents, labels, classes, smoothing=1):\n",
    "    # Total number of documents\n",
    "    n_doc = len(documents)\n",
    "    # List of all the word types in the documents\n",
    "    vocab = set([word for doc_id in documents for word in documents[doc_id]])\n",
    "    vocab_size = len(vocab)\n",
    "    print(\"Number of word types %d \\n\" % len(vocab))\n",
    "    \n",
    "    prior, likelihood = {}, {}\n",
    "    # Initialize likelihood for each class\n",
    "    for c in classes:\n",
    "        likelihood[c] = {}\n",
    "    for c in classes:\n",
    "        # Number of documents labeled c\n",
    "        n_c = list(labels.values()).count(c)\n",
    "        # The (log) prior probability of class c: n_c/n_doc\n",
    "        prior[c] = np.log(n_c) - np.log(n_doc)\n",
    "        print(\"C %d, prior prob. %.2f\" % (c, prior[c]))\n",
    "        \n",
    "        # Frequency of each word in all documents with label c\n",
    "        doc_c = {}\n",
    "        for doc_id in documents:\n",
    "            if labels[doc_id] == c:  # Document has label c\n",
    "                for word in documents[doc_id]:\n",
    "                    if word not in doc_c:\n",
    "                        doc_c[word] = 0\n",
    "                    doc_c[word] += 1\n",
    "        print(\"Number of word types in c %d\" % len(doc_c))\n",
    "        \n",
    "        # Sum of all the word counts in c\n",
    "        denom = np.sum(list(doc_c.values())) + vocab_size * smoothing\n",
    "        denom = np.log(denom)\n",
    "        for word in vocab:\n",
    "            # Is the sum correct?\n",
    "            word_count = 0\n",
    "            if word in doc_c:\n",
    "                word_count = doc_c[word]\n",
    "            \n",
    "            likelihood[c][word] = np.log(word_count + smoothing) - denom \n",
    "\n",
    "            #print(c, word, likelihood[c][word])\n",
    "    return prior, likelihood, vocab\n",
    "\n",
    "prior, likelihood, vocab = naive_bayes(training_data, valence_labels, [-1, 1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 -7.38687329667\n",
      "1 -8.37780752233\n",
      "-1 -8.08002047723\n",
      "1 -6.99151316121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{-1: -8.0800204772250126, 1: -6.9915131612075356}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(testdoc, classes, prior, likelihood, vocab):\n",
    "    posterior = {}\n",
    "    for c in classes:\n",
    "        posterior[c] = prior[c]\n",
    "        for word in testdoc:\n",
    "            word = word.lower()\n",
    "            if word in vocab:\n",
    "                #print(word)\n",
    "                posterior[c] += likelihood[c][word]\n",
    "        print(c, posterior[c])\n",
    "    return posterior\n",
    "\n",
    "classify([\"I\", \"love\", \"Bayes\", \"rule\"], [-1, 1], prior, likelihood, vocab)\n",
    "\n",
    "classify([\"happy\", \"valentine's\", \"day\"], [-1, 1], prior, likelihood, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Results\n",
    "\n",
    "* Remove the stop words (Highly frequent words or function words)\n",
    "* Binarize the frequency of words\n",
    "* Deal with negation\n",
    "* Change the smoothing parameter\n",
    "\n",
    "# Experiment with Emotions\n",
    "\n",
    "The same dataset also includes an emotion corpora, where each document is annotated with scores for emotions such as joy. Train a mulit-class Naive Bayes using the emotion corpora.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
